{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.spatial.distance as distance\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_kernel(h, x, x_i):\n",
    "\n",
    "    \"\"\"\n",
    "    h: window width\n",
    "    x: point x for density estimation\n",
    "    x_i: point from training data set\n",
    "    \"\"\"\n",
    "    return (x -x_i) / h\n",
    "\n",
    "def parzen_window(x, h=1):  \n",
    "   \n",
    "    if (np.max(x) > h/2):\n",
    "        return 0\n",
    "    return 1 \n",
    "\n",
    "def guassian_window(x, h=1):\n",
    "\n",
    "    return np.sqrt(2*np.pi*h**2)**(-1) * np.exp(-(np.sum(x**2))/2)\n",
    "\n",
    "def parzen_estimation(x_samples, point_x, d, h, window_func, kernel_func):\n",
    "    k_n = 0 \n",
    "    #print(x_samples.shape)\n",
    "    for row in x_samples:\n",
    "        x_i = kernel_func(h, point_x[:,np.newaxis], row[:,np.newaxis])\n",
    "        k_n += window_func(x_i, h)\n",
    "    #print(h)\n",
    "    #print(\"**\")       \n",
    "    return k_n / (x_samples.shape[0] * h ** d)\n",
    "\n",
    "def distribution_estimation_parzen(all_data, test_data, h, class_number, parzen_window, hypercube_kernel):\n",
    "    n = test_data.shape[0]\n",
    "    d = test_data.shape[1]\n",
    "    count = -1\n",
    "    all_estimations = np.zeros((n, class_number))\n",
    "    for test in test_data:\n",
    "        count += 1\n",
    "        estimations = []\n",
    "        for i in range(class_number):\n",
    "            class_data = all_data[i]\n",
    "            estimations.append(parzen_estimation(class_data[0], test, d, h, parzen_window, hypercube_kernel))\n",
    "        all_estimations[count,:] = estimations\n",
    "    return all_estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier(object):\n",
    "    def __init__(self, PDs, Priors = None, Risk = None):\n",
    "        '''\n",
    "        PDs: array of PD object\n",
    "        Priors: arrays of prior probabilities\n",
    "        Risk: Risk matrix\n",
    "        NOTE: all PDs should have the same input dim and Risk mat shape should be (dim, dim)\n",
    "        '''\n",
    "        self.PDs = PDs\n",
    "        self.Priors = Priors\n",
    "        self.Risk = Risk\n",
    "        self.C = len(PDs) # number of classes\n",
    "\n",
    "        if self.Risk is None:\n",
    "            self.init_Risk()\n",
    "        if self.Priors is None:\n",
    "            self.init_Prios()\n",
    "    \n",
    "    def init_Risk(self):\n",
    "        self.Risk = np.ones((self.C, self.C)) - np.eye(self.C)\n",
    "\n",
    "    def init_Prios(self):\n",
    "        self.Priors = np.array([1 for i in range(self.C)])\n",
    "\n",
    "    def classify(self, x):\n",
    "        prs = np.array([PD for PD in self.PDs]).T * self.Priors\n",
    "        discs = np.dot(prs, self.Risk)\n",
    "        return np.argmin(discs, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.loadtxt(\"testData.csv\", delimiter=',', unpack=True)\n",
    "test_labels = np.loadtxt(\"testLabels.csv\", delimiter=',', unpack=True)\n",
    "train_data = np.loadtxt(\"trainData.csv\", delimiter=',', unpack=True)\n",
    "train_labels = np.loadtxt(\"trainLabels.csv\", delimiter=',', unpack=True)\n",
    "\n",
    "test_data = test_data.T\n",
    "train_data = train_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "tr_samples_size, _ = train_data.shape\n",
    "all_data = np.vstack((train_data,test_data))\n",
    "sel = VarianceThreshold(threshold=0.90*(1-0.90))\n",
    "all_data = sel.fit_transform(all_data)\n",
    "train_data = all_data[:tr_samples_size]\n",
    "test_data = all_data[tr_samples_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in range(class_number):\n",
    "    cl = train_data[np.where(train_labels == i), :]\n",
    "    all_data.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parzen classifie: hypercube kernel\n",
      "h : 0.25\n",
      "correct confusion rate :  0.2348\n",
      "probability of classification error :  0.7652\n",
      "*****\n",
      "h : 1\n",
      "correct confusion rate :  0.6772\n",
      "probability of classification error :  0.3228\n",
      "*****\n",
      "h : 2\n",
      "correct confusion rate :  0.0876\n",
      "probability of classification error :  0.9124\n",
      "*****\n",
      "h : 9\n",
      "correct confusion rate :  0.0876\n",
      "probability of classification error :  0.9124\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "class_number = 10\n",
    "print(\"parzen classifie: hypercube kernel\")\n",
    "parzen_rate = np.zeros(4)\n",
    "count = 0\n",
    "for i in [0.25, 1, 2, 9]:\n",
    "    all_estimations = distribution_estimation_parzen(all_data, test_data, i, class_number,parzen_window, hypercube_kernel)       \n",
    "    class_density_estimation = all_estimations.T  \n",
    "    classifier = BayesClassifier(class_density_estimation)\n",
    "    t_labels = classifier.classify(test_data)\n",
    "    cnf_matrix = confusion_matrix(test_labels, t_labels)\n",
    "    parzen_rate[count] = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "    print(\"h :\", i)\n",
    "    print(\"correct confusion rate : \", parzen_rate[count])\n",
    "    print(\"probability of classification error : \", 1-parzen_rate[count])\n",
    "    print(\"*****\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parzen classifie: Gaussian kernel\n",
      "h : 0.25\n",
      "correct confusion rate :  0.2348\n",
      "probability of classification error :  0.7652\n",
      "*****\n",
      "h : 1\n",
      "correct confusion rate :  0.6772\n",
      "probability of classification error :  0.3228\n",
      "*****\n",
      "h : 2\n",
      "correct confusion rate :  0.0876\n",
      "probability of classification error :  0.9124\n",
      "*****\n",
      "h : 9\n",
      "correct confusion rate :  0.0876\n",
      "probability of classification error :  0.9124\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "print(\"parzen classifie: Gaussian kernel\")    \n",
    "guassian_rate = np.zeros(4)\n",
    "count = 0\n",
    "for i in [0.25, 1, 2, 9]:\n",
    "    all_estimations = distribution_estimation_parzen(all_data, test_data, i, class_number, guassian_window, hypercube_kernel)       \n",
    "    class_density_estimation = all_estimations.T  \n",
    "    classifier = BayesClassifier(class_density_estimation)\n",
    "    t_labels = classifier.classify(test_data)\n",
    "    cnf_matrix = confusion_matrix(test_labels, t_labels)\n",
    "    guassian_rate[count] = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "    print(\"h :\", i)\n",
    "    print(\"correct confusion rate : \", parzen_rate[count])\n",
    "    print(\"probability of classification error : \", 1-parzen_rate[count])\n",
    "    print(\"*****\")\n",
    "    count += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(x, data, labels, K, class_number):\n",
    "    num_samples = data.shape[0]\n",
    "    distances = np.array([distance.euclidean(x ,data[i,:]) for i in range(num_samples)])\n",
    "    index = [i[0] for i in sorted(enumerate(distances), key = lambda x:x[1])[:K]]\n",
    "    labl = labels[index]\n",
    "    labl = labl[:,np.newaxis]\n",
    "    predicts = np.zeros((1,class_number))\n",
    "    \n",
    "    for i in range(class_number):\n",
    "        predicts[:,int(i)] = np.array(np.where(labl == i))[0].shape[0]\n",
    "    return predicts \n",
    "\n",
    "def distribution_estimation(test_data, t_data, train_label, K, class_number):\n",
    "    dist = np.zeros((test_data.shape[0],class_number))\n",
    "    count = 0\n",
    "    for sample in test_data:\n",
    "        dist[count,:] = predict_knn(sample, t_data, train_label, K, class_number)\n",
    "        count += 1\n",
    "    return dist/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn classifier\n",
      "k : 1\n",
      "correct confusion rate :  0.9248\n",
      "probability of classification error :  0.07520000000000004\n",
      "*****\n",
      "k : 5\n",
      "correct confusion rate :  0.9056\n",
      "probability of classification error :  0.09440000000000004\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "print(\"knn classifier\")\n",
    "k_nearest_rate = np.zeros(3)\n",
    "count = 0\n",
    "for k in [1, 5, 10]:\n",
    "    class_density_estimation = distribution_estimation(test_data, train_data, train_labels, k, class_number)\n",
    "    class_density_estimation = class_density_estimation.T \n",
    "    classifier = BayesClassifier(class_density_estimation)\n",
    "    t_labels = classifier.classify(test_data) \n",
    "    cnf_matrix = confusion_matrix(test_labels, t_labels)\n",
    "    k_nearest_rate[count] = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "    print(\"k :\", k)\n",
    "    print(\"correct confusion rate : \", k_nearest_rate[count])\n",
    "    print(\"probability of classification error : \", 1-k_nearest_rate[count])\n",
    "    print(\"*****\")\n",
    "    count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"knn classifier\")\n",
    "count = 0\n",
    "k_correct_rate = np.zeros(6)\n",
    "for i in [3000, 5000]:\n",
    "    print(\"train set size: \", i)\n",
    "    for k in [10, 15]:\n",
    "        class_density_estimation = distribution_estimation(test_data, train_data[0:i,:], train_labels[0:i], k, class_number)\n",
    "        class_density_estimation = class_density_estimation.T \n",
    "        classifier = BayesClassifier(class_density_estimation)\n",
    "        t_labels = classifier.classify(test_data)\n",
    "        where = np.where((t_labels-test_labels) == 0)[0]\n",
    "        k_correct_rate[count] = where.shape[0]/test_labels.shape[0]\n",
    "        #cnf_matrix = confusion_matrix(test_labels[0:100], t_labels)\n",
    "        #k_correct_rate = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "        print(\"k :\", k)\n",
    "        print(\"correct confusion rate : \", k_correct_rate[count])\n",
    "        print(\"probability of classification error : \", 1-k_correct_rate[count])\n",
    "        print(\"*****\")\n",
    "        count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"parzen classifier\")\n",
    "\n",
    "count = 0\n",
    "h_correct_rate = np.zeros(6)\n",
    "for i in [3000, 5000]:\n",
    "    print(\"train set size: \", i)\n",
    "    for h in [1, 3]:    \n",
    "        all_estimations = distribution_estimation_parzen(all_data, test_data, h, class_number, parzen_window, hypercube_kernel)       \n",
    "        class_density_estimation = all_estimations.T  \n",
    "        classifier = BayesClassifier(class_density_estimation)\n",
    "        t_labels = classifier.classify(test_data)\n",
    "        where = np.where((t_labels-test_labels) == 0)[0]\n",
    "        h_correct_rate[count] = where.shape[0]/test_labels.shape[0]\n",
    "        #cnf_matrix = confusion_matrix(test_labels, t_labels)\n",
    "        #h_correct_rate[count] = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "        print(\"h :\", h)\n",
    "        print(\"correct confusion rate : \", parzen_rate[count])\n",
    "        print(\"probability of classification error : \", 1-parzen_rate[count])\n",
    "        print(\"*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((test_data, train_data))\n",
    "                      \n",
    "mean_vec = np.mean(train_data, axis=0)\n",
    "cov_mat = np.cov(train_data, rowvar=False)\n",
    "#cov_mat = (data - mean_vec).T.dot((data - mean_vec)) / (data.shape[0]-1)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "#u,s,v = np.linalg.svd(train_data)\n",
    "\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(62), var_exp, alpha=.5, align='center', label='individual explained variance')\n",
    "    plt.step(range(62), cum_var_exp, where='mid', label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_w = np.array(eig_pairs)[0:40, 1]\n",
    "matrix_w = matrix_w.tolist()\n",
    "matrix_w = np.reshape(matrix_w, (40, 62)).T\n",
    "transformed = data.dot(matrix_w)\n",
    "\n",
    "test_data = transformed[0:2500, :]\n",
    "train_data = transformed[2500:, :]\n",
    "test_data = np.asarray(test_data)\n",
    "train_data = np.asarray(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in range(class_number):\n",
    "    cl = train_data[np.where(train_labels == i), :]\n",
    "    all_data.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"knn classifier\")\n",
    "class_density_estimation = distribution_estimation(test_data, train_data, train_labels, 10, class_number)\n",
    "class_density_estimation = class_density_estimation.T \n",
    "classifier = BayesClassifier(class_density_estimation)\n",
    "t_labels = classifier.classify(test_data)\n",
    "cnf_matrix = confusion_matrix(test_labels, test_labels)\n",
    "k_correct_rate = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "print(\"k :\", 10)\n",
    "print(\"correct confusion rate : \", k_correct_rate)\n",
    "print(\"probability of classification error : \", 1-k_correct_rate)\n",
    "print(\"*****\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"parzen classifier\") \n",
    "all_estimations = distribution_estimation_parzen(all, test_data, 1, class_number, parzen_window, hypercube_kernel)       \n",
    "class_density_estimation = all_estimations.T  \n",
    "classifier = BayesClassifier(class_density_estimation)\n",
    "t_labels = classifier.classify(test_data)\n",
    "cnf_matrix = confusion_matrix(test_labels, test_labels)\n",
    "h_correct_rate = np.sum(np.diag(cnf_matrix))/test_labels.shape\n",
    "print(\"h :\", 1)\n",
    "print(\"correct confusion rate : \", h_correct_rate)\n",
    "print(\"probability of classification error : \", 1-h_correct_rate)\n",
    "print(\"*****\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
